{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b89f64d8f8053d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 单卡GPU 进行 ChatGLM3-6B模型 LORA 高效微调\n",
    "本 Cookbook 将带领开发者使用 `AdvertiseGen` 对 ChatGLM3-6B 数据集进行 lora微调，使其具备专业的广告生成能力。\n",
    "\n",
    "## 硬件需求\n",
    "显存：24GB\n",
    "显卡架构：安培架构（推荐）\n",
    "内存：16GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd9a514ed09ea6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1. 准备数据集\n",
    "我们使用 AdvertiseGen 数据集来进行微调。从 [Google Drive](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing) 或者 [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) 下载处理好的 AdvertiseGen 数据集，将解压后的 AdvertiseGen 目录放到本目录的 `/data/` 下, 例如。\n",
    "> /media/zr/Data/Code/ChatGLM3/finetune_demo/data/AdvertiseGen\n",
    "\n",
    "接着，运行本代码来切割数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T05:02:34.749308Z",
     "start_time": "2024-01-18T05:02:25.564458Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()\n",
    "\n",
    "\n",
    "def _mkdir(dir_name: Union[str, Path]):\n",
    "    dir_name = _resolve_path(dir_name)\n",
    "    if not dir_name.is_dir():\n",
    "        dir_name.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "\n",
    "def convert_adgen(data_dir: Union[str, Path], save_dir: Union[str, Path]):\n",
    "    def _convert(in_file: Path, out_file: Path):\n",
    "        _mkdir(out_file.parent)\n",
    "        with open(in_file, encoding='utf-8') as fin:\n",
    "            with open(out_file, 'wt', encoding='utf-8') as fout:\n",
    "                for line in fin:\n",
    "                    dct = json.loads(line)\n",
    "                    sample = {'conversations': [{'role': 'user', 'content': dct['content']},\n",
    "                                                {'role': 'assistant', 'content': dct['summary']}]}\n",
    "                    fout.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    data_dir = _resolve_path(data_dir)\n",
    "    save_dir = _resolve_path(save_dir)\n",
    "\n",
    "    train_file = data_dir / 'train.json'\n",
    "    if train_file.is_file():\n",
    "        out_file = save_dir / train_file.relative_to(data_dir)\n",
    "        _convert(train_file, out_file)\n",
    "\n",
    "    dev_file = data_dir / 'dev.json'\n",
    "    if dev_file.is_file():\n",
    "        out_file = save_dir / dev_file.relative_to(data_dir)\n",
    "        _convert(dev_file, out_file)\n",
    "\n",
    "\n",
    "convert_adgen('data/AdvertiseGen', 'data/AdvertiseGen_fix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7a99923349056",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2. 使用命令行开始微调,我们使用 lora 进行微调\n",
    "接着，我们仅需要将配置好的参数以命令行的形式传参给程序，就可以使用命令行进行高效微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c87410a24d844f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T06:44:56.043246Z",
     "start_time": "2024-01-18T05:05:28.425374Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:05<00:00,  1.31it/s]\n",
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.031217444255383614\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 114599\n",
      "})\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "--> Sanity check\n",
      "           '[gMASK]': 64790 -> -100\n",
      "               'sop': 64792 -> -100\n",
      "          '<|user|>': 64795 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '\\n': 13 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '类型': 33467 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '版': 55090 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '宽松': 40833 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '风格': 32799 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '性感': 40589 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '图案': 37505 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '线条': 37216 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '阔': 56529 -> -100\n",
      "                 '腿': 56158 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "     '<|assistant|>': 64796 -> -100\n",
      "                  '': 30910 -> 30910\n",
      "                '\\n': 13 -> 13\n",
      "                  '': 30910 -> 30910\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '阔': 56529 -> 56529\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '这': 54551 -> 54551\n",
      "                '两年': 33808 -> 33808\n",
      "                '真的': 32041 -> 32041\n",
      "                 '吸': 55360 -> 55360\n",
      "                 '粉': 55486 -> 55486\n",
      "                '不少': 32138 -> 32138\n",
      "                 '，': 31123 -> 31123\n",
      "                '明星': 32943 -> 32943\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '达': 54880 -> 54880\n",
      "                '人的': 31664 -> 31664\n",
      "                '心头': 46565 -> 46565\n",
      "                 '爱': 54799 -> 54799\n",
      "                 '。': 31155 -> 31155\n",
      "                '毕竟': 33051 -> 33051\n",
      "                 '好': 54591 -> 54591\n",
      "                 '穿': 55432 -> 55432\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '，': 31123 -> 31123\n",
      "                 '谁': 55622 -> 55622\n",
      "                '都能': 32904 -> 32904\n",
      "                 '穿': 55432 -> 55432\n",
      "                 '出': 54557 -> 54557\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '长': 54625 -> 54625\n",
      "                 '2': 30943 -> 30943\n",
      "                 '米': 55055 -> 55055\n",
      "               '的效果': 35590 -> 35590\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '，': 31123 -> 31123\n",
      "               '当然是': 48466 -> 48466\n",
      "                 '遮': 57148 -> 57148\n",
      "                 '肉': 55343 -> 55343\n",
      "                 '小': 54603 -> 54603\n",
      "                '能手': 49355 -> 49355\n",
      "                 '啊': 55674 -> 55674\n",
      "                 '。': 31155 -> 31155\n",
      "                '上身': 51605 -> 51605\n",
      "                 '随': 55119 -> 55119\n",
      "                 '性': 54642 -> 54642\n",
      "                '自然': 31799 -> 31799\n",
      "                 '不': 54535 -> 54535\n",
      "                 '拘': 57036 -> 57036\n",
      "                 '束': 55625 -> 55625\n",
      "                 '，': 31123 -> 31123\n",
      "                '面料': 46839 -> 46839\n",
      "                 '亲': 55113 -> 55113\n",
      "                 '肤': 56089 -> 56089\n",
      "                '舒适': 33894 -> 33894\n",
      "                 '贴': 55778 -> 55778\n",
      "                '身体': 31902 -> 31902\n",
      "                 '验': 55017 -> 55017\n",
      "                 '感': 54706 -> 54706\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '哒': 59230 -> 59230\n",
      "                 '。': 31155 -> 31155\n",
      "                 '系': 54712 -> 54712\n",
      "                 '带': 54882 -> 54882\n",
      "                '部分': 31726 -> 31726\n",
      "                '增加': 31917 -> 31917\n",
      "                '设计': 31735 -> 31735\n",
      "                '看点': 45032 -> 45032\n",
      "                 '，': 31123 -> 31123\n",
      "                 '还': 54656 -> 54656\n",
      "                 '让': 54772 -> 54772\n",
      "                '单品': 46539 -> 46539\n",
      "               '的设计': 34481 -> 34481\n",
      "                 '感': 54706 -> 54706\n",
      "                '更强': 43084 -> 43084\n",
      "                 '。': 31155 -> 31155\n",
      "                '腿部': 46799 -> 46799\n",
      "                '线条': 37216 -> 37216\n",
      "                 '若': 55351 -> 55351\n",
      "                 '隐': 55733 -> 55733\n",
      "                 '若': 55351 -> 55351\n",
      "                 '现': 54600 -> 54600\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                '性感': 40589 -> 40589\n",
      "                 '撩': 58521 -> 58521\n",
      "                 '人': 54533 -> 54533\n",
      "                 '。': 31155 -> 31155\n",
      "                '颜色': 33692 -> 33692\n",
      "                 '敲': 57004 -> 57004\n",
      "                '温柔': 34678 -> 34678\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                 '与': 54619 -> 54619\n",
      "                '裤子': 44722 -> 44722\n",
      "                '本身': 32754 -> 32754\n",
      "                 '所': 54626 -> 54626\n",
      "                '呈现': 33169 -> 33169\n",
      "               '的风格': 48084 -> 48084\n",
      "                '有点': 33149 -> 33149\n",
      "                 '反': 54955 -> 54955\n",
      "                 '差': 55342 -> 55342\n",
      "                 '萌': 56842 -> 56842\n",
      "                 '。': 31155 -> 31155\n",
      "                  '': 2 -> 2\n",
      "/root/miniconda3/envs/chatglm3/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "***** Running training *****\n",
      "  Num examples = 114,599\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 33,000\n",
      "  Number of trainable parameters = 1,949,696\n",
      "{'loss': 4.1619, 'grad_norm': 3.916167974472046, 'learning_rate': 4.984848484848485e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6257, 'grad_norm': 5.616059303283691, 'learning_rate': 4.9696969696969694e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5798, 'grad_norm': 5.493468284606934, 'learning_rate': 4.9545454545454553e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5031, 'grad_norm': 6.945895671844482, 'learning_rate': 4.93939393939394e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4638, 'grad_norm': 7.547438621520996, 'learning_rate': 4.9242424242424245e-05, 'epoch': 0.01}\n",
      "  2%|▌                                   | 500/33000 [14:17<15:07:58,  1.68s/it]Saving model checkpoint to ./output/checkpoint-500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 3.4976, 'grad_norm': 8.075969696044922, 'learning_rate': 4.909090909090909e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4099, 'grad_norm': 7.938879489898682, 'learning_rate': 4.8939393939393944e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4067, 'grad_norm': 7.271107196807861, 'learning_rate': 4.878787878787879e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4419, 'grad_norm': 8.294351577758789, 'learning_rate': 4.863636363636364e-05, 'epoch': 0.02}\n",
      "{'loss': 3.391, 'grad_norm': 8.299139976501465, 'learning_rate': 4.848484848484849e-05, 'epoch': 0.02}\n",
      "  3%|█                                  | 1000/33000 [28:33<17:44:56,  2.00s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:07<00:43,  3.91s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:15<00:52,  5.30s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:23<00:57,  6.42s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:35<01:06,  8.29s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [00:44<01:01,  8.72s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [00:54<00:54,  9.15s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [01:50<01:59, 23.87s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [01:57<01:14, 18.70s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [02:04<00:44, 14.99s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [02:11<00:25, 12.52s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [02:16<00:10, 10.39s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 13/13 [02:22<00:00,  8.85s/it]\u001b[ABuilding prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.835 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                                \n",
      "\u001b[A{'eval_rouge-1': 30.499078000000008, 'eval_rouge-2': 6.339848, 'eval_rouge-l': 23.746772, 'eval_bleu-4': 0.029744749464597504, 'eval_runtime': 150.6217, 'eval_samples_per_second': 0.332, 'eval_steps_per_second': 0.086, 'epoch': 0.02}\n",
      "  3%|█                                  | 1000/33000 [31:04<17:44:56,  2.00s/it]\n",
      "100%|███████████████████████████████████████████| 13/13 [02:23<00:00,  8.85s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-1000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 3.4076, 'grad_norm': 8.013458251953125, 'learning_rate': 4.8333333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4034, 'grad_norm': 8.123400688171387, 'learning_rate': 4.8181818181818186e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3824, 'grad_norm': 7.607954502105713, 'learning_rate': 4.803030303030303e-05, 'epoch': 0.02}\n",
      "{'loss': 3.335, 'grad_norm': 8.277449607849121, 'learning_rate': 4.787878787878788e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3629, 'grad_norm': 8.194007873535156, 'learning_rate': 4.772727272727273e-05, 'epoch': 0.03}\n",
      "  5%|█▌                                 | 1500/33000 [45:08<13:33:17,  1.55s/it]Saving model checkpoint to ./output/checkpoint-1500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1500/special_tokens_map.json\n",
      "{'loss': 3.4071, 'grad_norm': 7.214837074279785, 'learning_rate': 4.7575757575757576e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3222, 'grad_norm': 7.615163326263428, 'learning_rate': 4.742424242424243e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3249, 'grad_norm': 8.942221641540527, 'learning_rate': 4.7272727272727275e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3775, 'grad_norm': 8.239201545715332, 'learning_rate': 4.712121212121212e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3394, 'grad_norm': 8.27745532989502, 'learning_rate': 4.696969696969697e-05, 'epoch': 0.03}\n",
      "  6%|██                                 | 2000/33000 [59:18<15:18:00,  1.78s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:06<00:37,  3.41s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:12<00:44,  4.42s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:19<00:49,  5.49s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:26<00:47,  5.91s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [00:36<00:50,  7.27s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [00:47<00:50,  8.39s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [00:54<00:40,  8.05s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [01:01<00:31,  7.76s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [01:08<00:22,  7.47s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [01:16<00:15,  7.60s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [01:23<00:07,  7.37s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.561499999999995, 'eval_rouge-2': 5.941464, 'eval_rouge-l': 25.059376, 'eval_bleu-4': 0.028985929878771674, 'eval_runtime': 144.1486, 'eval_samples_per_second': 0.347, 'eval_steps_per_second': 0.09, 'epoch': 0.03}\n",
      "  6%|██                               | 2000/33000 [1:01:42<15:18:00,  1.78s/it]\n",
      "100%|███████████████████████████████████████████| 13/13 [01:28<00:00,  6.65s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-2000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2000/special_tokens_map.json\n",
      "{'loss': 3.3867, 'grad_norm': 10.487040519714355, 'learning_rate': 4.681818181818182e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3566, 'grad_norm': 8.244901657104492, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3051, 'grad_norm': 8.426569938659668, 'learning_rate': 4.651515151515152e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3247, 'grad_norm': 7.955988883972168, 'learning_rate': 4.636363636363636e-05, 'epoch': 0.04}\n",
      "{'loss': 3.2769, 'grad_norm': 7.913094997406006, 'learning_rate': 4.621212121212121e-05, 'epoch': 0.04}\n",
      "  8%|██▌                              | 2500/33000 [1:15:46<15:54:28,  1.88s/it]Saving model checkpoint to ./output/checkpoint-2500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2500/special_tokens_map.json\n",
      "{'loss': 3.31, 'grad_norm': 8.037178039550781, 'learning_rate': 4.606060606060607e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3165, 'grad_norm': 7.639129638671875, 'learning_rate': 4.5909090909090914e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3199, 'grad_norm': 6.750209331512451, 'learning_rate': 4.575757575757576e-05, 'epoch': 0.05}\n",
      "{'loss': 3.2416, 'grad_norm': 7.5129008293151855, 'learning_rate': 4.5606060606060606e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3297, 'grad_norm': 7.795763969421387, 'learning_rate': 4.545454545454546e-05, 'epoch': 0.05}\n",
      "  9%|███                              | 3000/33000 [1:29:55<12:58:20,  1.56s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:08<00:44,  4.06s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:13<00:46,  4.67s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:21<00:51,  5.68s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:29<00:52,  6.61s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [00:36<00:48,  6.91s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [00:48<00:51,  8.55s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [00:57<00:43,  8.68s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [01:05<00:33,  8.28s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [02:00<01:07, 22.63s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [02:07<00:36, 18.08s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [02:16<00:15, 15.06s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.84932, 'eval_rouge-2': 6.431608, 'eval_rouge-l': 24.835092000000003, 'eval_bleu-4': 0.034198159116179676, 'eval_runtime': 151.3559, 'eval_samples_per_second': 0.33, 'eval_steps_per_second': 0.086, 'epoch': 0.05}\n",
      "  9%|███                              | 3000/33000 [1:32:26<12:58:20,  1.56s/it]\n",
      "100%|███████████████████████████████████████████| 13/13 [02:23<00:00, 12.75s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-3000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3000/special_tokens_map.json\n",
      "{'loss': 3.3288, 'grad_norm': 7.8509135246276855, 'learning_rate': 4.5303030303030304e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3526, 'grad_norm': 7.746592998504639, 'learning_rate': 4.515151515151516e-05, 'epoch': 0.06}\n",
      "{'loss': 3.2954, 'grad_norm': 7.215171813964844, 'learning_rate': 4.5e-05, 'epoch': 0.06}\n",
      "{'loss': 3.322, 'grad_norm': 7.303887844085693, 'learning_rate': 4.484848484848485e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3567, 'grad_norm': 7.259897232055664, 'learning_rate': 4.46969696969697e-05, 'epoch': 0.06}\n",
      " 11%|███▌                             | 3500/33000 [1:46:38<14:23:30,  1.76s/it]Saving model checkpoint to ./output/checkpoint-3500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3500/special_tokens_map.json\n",
      "{'loss': 3.3028, 'grad_norm': 7.87929105758667, 'learning_rate': 4.454545454545455e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3379, 'grad_norm': 10.494933128356934, 'learning_rate': 4.43939393939394e-05, 'epoch': 0.06}\n",
      "{'loss': 3.2779, 'grad_norm': 8.293780326843262, 'learning_rate': 4.4242424242424246e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3205, 'grad_norm': 7.555900573730469, 'learning_rate': 4.409090909090909e-05, 'epoch': 0.07}\n",
      "{'loss': 3.2677, 'grad_norm': 7.947726249694824, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.07}\n",
      " 12%|████                             | 4000/33000 [2:00:40<13:00:22,  1.61s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:06<00:37,  3.41s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:12<00:44,  4.46s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [01:08<03:32, 23.62s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [01:18<02:29, 18.73s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [01:27<01:50, 15.80s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [01:40<01:27, 14.61s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [02:36<02:18, 27.61s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [02:44<01:26, 21.59s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [03:39<01:35, 31.85s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [04:34<01:17, 38.97s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [04:43<00:29, 29.86s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.75689, 'eval_rouge-2': 7.091244, 'eval_rouge-l': 23.874180000000003, 'eval_bleu-4': 0.032253942226495066, 'eval_runtime': 300.8776, 'eval_samples_per_second': 0.166, 'eval_steps_per_second': 0.043, 'epoch': 0.07}\n",
      " 12%|████                             | 4000/33000 [2:05:41<13:00:22,  1.61s/it]\n",
      "100%|███████████████████████████████████████████| 13/13 [04:50<00:00, 22.82s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-4000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4000/special_tokens_map.json\n",
      "{'loss': 3.378, 'grad_norm': 8.770230293273926, 'learning_rate': 4.378787878787879e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3031, 'grad_norm': 8.49294662475586, 'learning_rate': 4.3636363636363636e-05, 'epoch': 0.07}\n",
      "{'loss': 3.2918, 'grad_norm': 7.250067234039307, 'learning_rate': 4.348484848484849e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3276, 'grad_norm': 8.175103187561035, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.08}\n",
      "{'loss': 3.2906, 'grad_norm': 8.309642791748047, 'learning_rate': 4.318181818181819e-05, 'epoch': 0.08}\n",
      " 14%|████▌                            | 4500/33000 [2:19:48<13:16:48,  1.68s/it]Saving model checkpoint to ./output/checkpoint-4500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-4500/special_tokens_map.json\n",
      "{'loss': 3.2933, 'grad_norm': 8.199845314025879, 'learning_rate': 4.303030303030303e-05, 'epoch': 0.08}\n",
      "{'loss': 3.2428, 'grad_norm': 8.30644416809082, 'learning_rate': 4.287878787878788e-05, 'epoch': 0.08}\n",
      "{'loss': 3.2934, 'grad_norm': 6.707663059234619, 'learning_rate': 4.2727272727272724e-05, 'epoch': 0.08}\n",
      "{'loss': 3.2658, 'grad_norm': 7.770931720733643, 'learning_rate': 4.257575757575758e-05, 'epoch': 0.09}\n",
      "{'loss': 3.2748, 'grad_norm': 11.506457328796387, 'learning_rate': 4.242424242424243e-05, 'epoch': 0.09}\n",
      " 15%|█████                            | 5000/33000 [2:33:59<12:38:56,  1.63s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:08<00:49,  4.47s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:16<00:56,  5.66s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:26<01:04,  7.18s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:34<01:00,  7.58s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [00:43<00:56,  8.14s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [01:39<02:20, 23.47s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [01:48<01:34, 18.80s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [01:55<01:01, 15.42s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [02:03<00:38, 13.00s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [02:14<00:24, 12.25s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [02:21<00:10, 10.70s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.783901999999998, 'eval_rouge-2': 6.805324, 'eval_rouge-l': 24.805419999999998, 'eval_bleu-4': 0.03491791688935888, 'eval_runtime': 203.8854, 'eval_samples_per_second': 0.245, 'eval_steps_per_second': 0.064, 'epoch': 0.09}\n",
      " 15%|█████                            | 5000/33000 [2:37:23<12:38:56,  1.63s/it]\n",
      "100%|███████████████████████████████████████████| 13/13 [02:28<00:00,  9.53s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-5000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5000/special_tokens_map.json\n",
      "{'loss': 3.246, 'grad_norm': 7.22932243347168, 'learning_rate': 4.2272727272727275e-05, 'epoch': 0.09}\n",
      "{'loss': 3.3379, 'grad_norm': 8.316523551940918, 'learning_rate': 4.212121212121212e-05, 'epoch': 0.09}\n",
      "{'loss': 3.3286, 'grad_norm': 7.6663031578063965, 'learning_rate': 4.196969696969697e-05, 'epoch': 0.09}\n",
      "{'loss': 3.2942, 'grad_norm': 9.090229034423828, 'learning_rate': 4.181818181818182e-05, 'epoch': 0.09}\n",
      "{'loss': 3.2887, 'grad_norm': 8.632684707641602, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.1}\n",
      " 17%|█████▌                           | 5500/33000 [2:51:30<12:09:45,  1.59s/it]Saving model checkpoint to ./output/checkpoint-5500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-5500/special_tokens_map.json\n",
      "{'loss': 3.2052, 'grad_norm': 8.303356170654297, 'learning_rate': 4.151515151515152e-05, 'epoch': 0.1}\n",
      "{'loss': 3.3123, 'grad_norm': 7.972207069396973, 'learning_rate': 4.1363636363636364e-05, 'epoch': 0.1}\n",
      "{'loss': 3.2636, 'grad_norm': 10.039554595947266, 'learning_rate': 4.1212121212121216e-05, 'epoch': 0.1}\n",
      "{'loss': 3.1981, 'grad_norm': 8.22421932220459, 'learning_rate': 4.106060606060606e-05, 'epoch': 0.1}\n",
      "{'loss': 3.2815, 'grad_norm': 7.901778221130371, 'learning_rate': 4.0909090909090915e-05, 'epoch': 0.1}\n",
      " 18%|██████                           | 6000/33000 [3:05:31<11:50:47,  1.58s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:07<00:43,  3.91s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:16<00:57,  5.71s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:23<00:57,  6.41s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:32<00:57,  7.15s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [00:40<00:52,  7.51s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [00:51<00:51,  8.53s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [01:03<00:49,  9.89s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [01:11<00:36,  9.09s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [01:19<00:26,  8.76s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [01:28<00:17,  8.84s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [01:36<00:08,  8.51s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.87471, 'eval_rouge-2': 7.028614, 'eval_rouge-l': 25.706844, 'eval_bleu-4': 0.03560392546356579, 'eval_runtime': 111.2729, 'eval_samples_per_second': 0.449, 'eval_steps_per_second': 0.117, 'epoch': 0.1}\n",
      " 18%|██████                           | 6000/33000 [3:07:22<11:50:47,  1.58s/it]\n",
      "100%|███████████████████████████████████████████| 13/13 [01:43<00:00,  8.18s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-6000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6000/special_tokens_map.json\n",
      "{'loss': 3.2566, 'grad_norm': 7.630887031555176, 'learning_rate': 4.075757575757576e-05, 'epoch': 0.11}\n",
      "{'loss': 3.2454, 'grad_norm': 7.5966691970825195, 'learning_rate': 4.0606060606060606e-05, 'epoch': 0.11}\n",
      "{'loss': 3.2611, 'grad_norm': 7.315024375915527, 'learning_rate': 4.045454545454546e-05, 'epoch': 0.11}\n",
      "{'loss': 3.2748, 'grad_norm': 7.92302131652832, 'learning_rate': 4.0303030303030305e-05, 'epoch': 0.11}\n",
      "{'loss': 3.283, 'grad_norm': 7.163276195526123, 'learning_rate': 4.015151515151515e-05, 'epoch': 0.11}\n",
      " 20%|██████▌                          | 6500/33000 [3:21:25<12:09:30,  1.65s/it]Saving model checkpoint to ./output/checkpoint-6500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-6500/special_tokens_map.json\n",
      "{'loss': 3.2532, 'grad_norm': 7.5635881423950195, 'learning_rate': 4e-05, 'epoch': 0.12}\n",
      "{'loss': 3.2282, 'grad_norm': 9.577190399169922, 'learning_rate': 3.984848484848485e-05, 'epoch': 0.12}\n",
      "{'loss': 3.2591, 'grad_norm': 7.949853420257568, 'learning_rate': 3.96969696969697e-05, 'epoch': 0.12}\n",
      "{'loss': 3.1849, 'grad_norm': 8.032187461853027, 'learning_rate': 3.954545454545455e-05, 'epoch': 0.12}\n",
      "{'loss': 3.2371, 'grad_norm': 7.153207778930664, 'learning_rate': 3.939393939393939e-05, 'epoch': 0.12}\n",
      " 21%|███████                          | 7000/33000 [3:35:29<12:05:54,  1.68s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:55<05:02, 27.51s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [01:04<03:19, 19.99s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [01:13<02:21, 15.74s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [01:22<01:48, 13.52s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [01:31<01:24, 12.01s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [01:42<01:09, 11.58s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [01:52<00:55, 11.07s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [02:01<00:41, 10.47s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [02:08<00:28,  9.56s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [02:16<00:17,  8.90s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [02:25<00:08,  8.92s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.29973, 'eval_rouge-2': 7.10943, 'eval_rouge-l': 24.801795999999996, 'eval_bleu-4': 0.036200943783514815, 'eval_runtime': 160.844, 'eval_samples_per_second': 0.311, 'eval_steps_per_second': 0.081, 'epoch': 0.12}\n",
      " 21%|███████                          | 7000/33000 [3:38:10<12:05:54,  1.68s/it]\n",
      "100%|███████████████████████████████████████████| 13/13 [02:31<00:00,  8.17s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-7000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7000/special_tokens_map.json\n",
      "{'loss': 3.2205, 'grad_norm': 7.779572010040283, 'learning_rate': 3.924242424242424e-05, 'epoch': 0.12}\n",
      "{'loss': 3.1777, 'grad_norm': 8.080215454101562, 'learning_rate': 3.909090909090909e-05, 'epoch': 0.13}\n",
      "{'loss': 3.2347, 'grad_norm': 8.77830696105957, 'learning_rate': 3.8939393939393944e-05, 'epoch': 0.13}\n",
      "{'loss': 3.2354, 'grad_norm': 7.368055820465088, 'learning_rate': 3.878787878787879e-05, 'epoch': 0.13}\n",
      "{'loss': 3.2017, 'grad_norm': 7.682941913604736, 'learning_rate': 3.8636363636363636e-05, 'epoch': 0.13}\n",
      " 23%|███████▌                         | 7500/33000 [3:52:18<13:05:12,  1.85s/it]Saving model checkpoint to ./output/checkpoint-7500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-7500/special_tokens_map.json\n",
      "{'loss': 3.1815, 'grad_norm': 7.684497833251953, 'learning_rate': 3.848484848484848e-05, 'epoch': 0.13}\n",
      "{'loss': 3.2016, 'grad_norm': 8.112489700317383, 'learning_rate': 3.8333333333333334e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3136, 'grad_norm': 7.412136077880859, 'learning_rate': 3.818181818181819e-05, 'epoch': 0.14}\n",
      "{'loss': 3.2525, 'grad_norm': 7.768249988555908, 'learning_rate': 3.803030303030303e-05, 'epoch': 0.14}\n",
      "{'loss': 3.2679, 'grad_norm': 9.013249397277832, 'learning_rate': 3.787878787878788e-05, 'epoch': 0.14}\n",
      " 24%|████████                         | 8000/33000 [4:06:28<10:41:39,  1.54s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:06<00:33,  3.06s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:13<00:47,  4.80s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:22<00:58,  6.52s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:31<00:58,  7.26s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [01:27<02:43, 23.33s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [01:36<01:52, 18.80s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [01:45<01:18, 15.76s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [01:53<00:53, 13.33s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [01:59<00:33, 11.17s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [02:07<00:20, 10.27s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [02:13<00:08,  8.86s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.376599999999996, 'eval_rouge-2': 7.600094, 'eval_rouge-l': 24.883770000000005, 'eval_bleu-4': 0.03475296668160657, 'eval_runtime': 195.4475, 'eval_samples_per_second': 0.256, 'eval_steps_per_second': 0.067, 'epoch': 0.14}\n",
      " 24%|████████                         | 8000/33000 [4:09:44<10:41:39,  1.54s/it]\n",
      "100%|███████████████████████████████████████████| 13/13 [02:19<00:00,  8.01s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-8000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8000/special_tokens_map.json\n",
      "{'loss': 3.2286, 'grad_norm': 8.085700035095215, 'learning_rate': 3.7727272727272725e-05, 'epoch': 0.14}\n",
      "{'loss': 3.2117, 'grad_norm': 8.099111557006836, 'learning_rate': 3.757575757575758e-05, 'epoch': 0.14}\n",
      "{'loss': 3.2242, 'grad_norm': 7.575295925140381, 'learning_rate': 3.742424242424243e-05, 'epoch': 0.14}\n",
      "{'loss': 3.2424, 'grad_norm': 7.201913833618164, 'learning_rate': 3.7272727272727276e-05, 'epoch': 0.15}\n",
      "{'loss': 3.2483, 'grad_norm': 7.55087423324585, 'learning_rate': 3.712121212121212e-05, 'epoch': 0.15}\n",
      " 26%|████████▌                        | 8500/33000 [4:23:59<13:40:15,  2.01s/it]Saving model checkpoint to ./output/checkpoint-8500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-8500/special_tokens_map.json\n",
      "{'loss': 3.1848, 'grad_norm': 6.898740291595459, 'learning_rate': 3.6969696969696974e-05, 'epoch': 0.15}\n",
      "{'loss': 3.178, 'grad_norm': 6.821269512176514, 'learning_rate': 3.681818181818182e-05, 'epoch': 0.15}\n",
      "{'loss': 3.1926, 'grad_norm': 7.725399971008301, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.15}\n",
      "{'loss': 3.1612, 'grad_norm': 6.837993144989014, 'learning_rate': 3.651515151515152e-05, 'epoch': 0.16}\n",
      "{'loss': 3.2516, 'grad_norm': 7.285558700561523, 'learning_rate': 3.6363636363636364e-05, 'epoch': 0.16}\n",
      " 27%|█████████                        | 9000/33000 [4:38:09<12:39:48,  1.90s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:08<00:45,  4.11s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:17<01:01,  6.17s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:26<01:05,  7.27s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:35<01:02,  7.78s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [01:30<02:45, 23.69s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [02:26<03:24, 34.02s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [02:36<02:12, 26.45s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [02:45<01:23, 20.86s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [02:51<00:49, 16.39s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [03:00<00:28, 14.08s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [03:55<00:26, 26.51s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.853156, 'eval_rouge-2': 6.89375, 'eval_rouge-l': 23.673794, 'eval_bleu-4': 0.03500062748694277, 'eval_runtime': 300.4467, 'eval_samples_per_second': 0.166, 'eval_steps_per_second': 0.043, 'epoch': 0.16}\n",
      " 27%|█████████                        | 9000/33000 [4:43:10<12:39:48,  1.90s/it]\n",
      "100%|███████████████████████████████████████████| 13/13 [04:04<00:00, 21.31s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-9000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9000/special_tokens_map.json\n",
      "{'loss': 3.1888, 'grad_norm': 8.41839599609375, 'learning_rate': 3.621212121212122e-05, 'epoch': 0.16}\n",
      "{'loss': 3.2037, 'grad_norm': 9.000229835510254, 'learning_rate': 3.606060606060606e-05, 'epoch': 0.16}\n",
      "{'loss': 3.209, 'grad_norm': 7.660837173461914, 'learning_rate': 3.590909090909091e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3106, 'grad_norm': 8.719832420349121, 'learning_rate': 3.575757575757576e-05, 'epoch': 0.16}\n",
      "{'loss': 3.2349, 'grad_norm': 8.674087524414062, 'learning_rate': 3.560606060606061e-05, 'epoch': 0.17}\n",
      " 29%|█████████▌                       | 9500/33000 [4:57:21<11:35:02,  1.77s/it]Saving model checkpoint to ./output/checkpoint-9500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-9500/special_tokens_map.json\n",
      "{'loss': 3.2343, 'grad_norm': 9.660877227783203, 'learning_rate': 3.545454545454546e-05, 'epoch': 0.17}\n",
      "{'loss': 3.1992, 'grad_norm': 10.638964653015137, 'learning_rate': 3.5303030303030305e-05, 'epoch': 0.17}\n",
      "{'loss': 3.1978, 'grad_norm': 7.866522789001465, 'learning_rate': 3.515151515151515e-05, 'epoch': 0.17}\n",
      "{'loss': 3.1709, 'grad_norm': 8.947835922241211, 'learning_rate': 3.5e-05, 'epoch': 0.17}\n",
      "{'loss': 3.1678, 'grad_norm': 8.297628402709961, 'learning_rate': 3.484848484848485e-05, 'epoch': 0.17}\n",
      " 30%|█████████▋                      | 10000/33000 [5:11:23<10:19:17,  1.62s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:06<00:38,  3.46s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:13<00:48,  4.82s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:21<00:54,  6.01s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:29<00:53,  6.67s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [00:39<00:54,  7.81s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [01:35<02:19, 23.25s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [01:43<01:31, 18.33s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [01:50<00:59, 14.81s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [02:45<01:21, 27.16s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [02:52<00:42, 21.10s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [02:57<00:16, 16.25s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.159721999999995, 'eval_rouge-2': 7.69043, 'eval_rouge-l': 22.207074000000002, 'eval_bleu-4': 0.03108808480354805, 'eval_runtime': 285.7687, 'eval_samples_per_second': 0.175, 'eval_steps_per_second': 0.045, 'epoch': 0.17}\n",
      " 30%|█████████▋                      | 10000/33000 [5:16:09<10:19:17,  1.62s/it]\n",
      "100%|███████████████████████████████████████████| 13/13 [03:50<00:00, 27.16s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-10000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-10000/special_tokens_map.json\n",
      "{'loss': 3.1813, 'grad_norm': 10.52181625366211, 'learning_rate': 3.46969696969697e-05, 'epoch': 0.18}\n",
      "{'loss': 3.2488, 'grad_norm': 7.345197677612305, 'learning_rate': 3.454545454545455e-05, 'epoch': 0.18}\n",
      "{'loss': 3.1771, 'grad_norm': 8.413860321044922, 'learning_rate': 3.4393939393939394e-05, 'epoch': 0.18}\n",
      "{'loss': 3.2064, 'grad_norm': 7.04478645324707, 'learning_rate': 3.424242424242424e-05, 'epoch': 0.18}\n",
      "{'loss': 3.2192, 'grad_norm': 8.001269340515137, 'learning_rate': 3.409090909090909e-05, 'epoch': 0.18}\n",
      " 32%|██████████▌                      | 10500/33000 [5:30:15<9:43:06,  1.55s/it]Saving model checkpoint to ./output/checkpoint-10500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-10500/special_tokens_map.json\n",
      "{'loss': 3.069, 'grad_norm': 7.757240295410156, 'learning_rate': 3.3939393939393945e-05, 'epoch': 0.18}\n",
      "{'loss': 3.1735, 'grad_norm': 7.436138153076172, 'learning_rate': 3.378787878787879e-05, 'epoch': 0.19}\n",
      "{'loss': 3.1536, 'grad_norm': 7.177408218383789, 'learning_rate': 3.3636363636363636e-05, 'epoch': 0.19}\n",
      "{'loss': 3.2079, 'grad_norm': 6.850039482116699, 'learning_rate': 3.348484848484848e-05, 'epoch': 0.19}\n",
      "{'loss': 3.1715, 'grad_norm': 7.870574951171875, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.19}\n",
      " 33%|██████████▋                     | 11000/33000 [5:44:31<10:29:26,  1.72s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:07<00:43,  3.97s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:14<00:52,  5.24s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [01:10<03:36, 24.11s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [01:19<02:29, 18.63s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [01:27<01:45, 15.11s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [01:36<01:19, 13.32s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [01:47<01:02, 12.43s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [01:54<00:43, 10.90s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [02:06<00:33, 11.27s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [02:15<00:20, 10.43s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [03:10<00:23, 23.97s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.091192, 'eval_rouge-2': 7.5659220000000005, 'eval_rouge-l': 24.129479999999994, 'eval_bleu-4': 0.033625615124891775, 'eval_runtime': 298.6042, 'eval_samples_per_second': 0.167, 'eval_steps_per_second': 0.044, 'epoch': 0.19}\n",
      " 33%|██████████▋                     | 11000/33000 [5:49:30<10:29:26,  1.72s/it]\n",
      "100%|███████████████████████████████████████████| 13/13 [04:03<00:00, 32.55s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-11000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-11000/special_tokens_map.json\n",
      "{'loss': 3.2154, 'grad_norm': 8.466094017028809, 'learning_rate': 3.318181818181819e-05, 'epoch': 0.19}\n",
      "{'loss': 3.1562, 'grad_norm': 8.345413208007812, 'learning_rate': 3.303030303030303e-05, 'epoch': 0.2}\n",
      "{'loss': 3.1941, 'grad_norm': 8.344799041748047, 'learning_rate': 3.287878787878788e-05, 'epoch': 0.2}\n",
      "{'loss': 3.2186, 'grad_norm': 8.428945541381836, 'learning_rate': 3.272727272727273e-05, 'epoch': 0.2}\n",
      "{'loss': 3.2291, 'grad_norm': 8.006743431091309, 'learning_rate': 3.257575757575758e-05, 'epoch': 0.2}\n",
      " 35%|███████████▌                     | 11500/33000 [6:03:45<9:28:14,  1.59s/it]Saving model checkpoint to ./output/checkpoint-11500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-11500/special_tokens_map.json\n",
      "{'loss': 3.2072, 'grad_norm': 8.115596771240234, 'learning_rate': 3.2424242424242423e-05, 'epoch': 0.2}\n",
      "{'loss': 3.1155, 'grad_norm': 6.812204360961914, 'learning_rate': 3.2272727272727276e-05, 'epoch': 0.2}\n",
      "{'loss': 3.0935, 'grad_norm': 8.069574356079102, 'learning_rate': 3.212121212121212e-05, 'epoch': 0.21}\n",
      "{'loss': 3.2786, 'grad_norm': 7.45078182220459, 'learning_rate': 3.1969696969696974e-05, 'epoch': 0.21}\n",
      "{'loss': 3.2526, 'grad_norm': 7.880958080291748, 'learning_rate': 3.181818181818182e-05, 'epoch': 0.21}\n",
      " 36%|████████████                     | 12000/33000 [6:18:06<9:41:02,  1.66s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:07<00:39,  3.61s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:13<00:47,  4.70s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [01:09<03:33, 23.77s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [02:04<04:37, 34.73s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [03:00<04:51, 41.69s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [03:12<03:12, 32.08s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [03:21<02:04, 24.96s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [03:28<01:18, 19.51s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [03:36<00:47, 15.95s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [03:44<00:26, 13.43s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [03:51<00:11, 11.37s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.301362, 'eval_rouge-2': 7.074089999999999, 'eval_rouge-l': 23.733361999999996, 'eval_bleu-4': 0.03342524532207175, 'eval_runtime': 296.0351, 'eval_samples_per_second': 0.169, 'eval_steps_per_second': 0.044, 'epoch': 0.21}\n",
      " 36%|████████████                     | 12000/33000 [6:23:02<9:41:02,  1.66s/it]\n",
      "100%|███████████████████████████████████████████| 13/13 [04:00<00:00, 10.63s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-12000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-12000/special_tokens_map.json\n",
      "{'loss': 3.1301, 'grad_norm': 8.44372844696045, 'learning_rate': 3.1666666666666666e-05, 'epoch': 0.21}\n",
      "{'loss': 3.1885, 'grad_norm': 8.418975830078125, 'learning_rate': 3.151515151515151e-05, 'epoch': 0.21}\n",
      "{'loss': 3.1783, 'grad_norm': 8.078632354736328, 'learning_rate': 3.1363636363636365e-05, 'epoch': 0.21}\n",
      "{'loss': 3.1594, 'grad_norm': 8.607256889343262, 'learning_rate': 3.121212121212122e-05, 'epoch': 0.22}\n",
      "{'loss': 3.1663, 'grad_norm': 9.489255905151367, 'learning_rate': 3.106060606060606e-05, 'epoch': 0.22}\n",
      " 38%|████████████▌                    | 12500/33000 [6:37:21<8:42:40,  1.53s/it]Saving model checkpoint to ./output/checkpoint-12500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-12500/special_tokens_map.json\n",
      "{'loss': 3.1711, 'grad_norm': 8.6531343460083, 'learning_rate': 3.090909090909091e-05, 'epoch': 0.22}\n",
      "{'loss': 3.2213, 'grad_norm': 9.219635963439941, 'learning_rate': 3.0757575757575755e-05, 'epoch': 0.22}\n",
      "{'loss': 3.258, 'grad_norm': 7.700500011444092, 'learning_rate': 3.060606060606061e-05, 'epoch': 0.22}\n",
      "{'loss': 3.2227, 'grad_norm': 6.501437664031982, 'learning_rate': 3.0454545454545456e-05, 'epoch': 0.23}\n",
      "{'loss': 3.2057, 'grad_norm': 8.191141128540039, 'learning_rate': 3.0303030303030306e-05, 'epoch': 0.23}\n",
      " 39%|█████████████                    | 13000/33000 [6:51:22<9:07:16,  1.64s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:55<05:02, 27.51s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [01:50<06:29, 38.98s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [01:57<04:05, 27.27s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [02:05<02:45, 20.65s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [02:14<01:57, 16.81s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [02:24<01:27, 14.57s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [02:33<01:04, 12.80s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [02:41<00:45, 11.35s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [02:49<00:30, 10.17s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [02:57<00:19,  9.54s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [03:03<00:08,  8.32s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 34.244706, 'eval_rouge-2': 8.500402, 'eval_rouge-l': 25.674338000000002, 'eval_bleu-4': 0.03970152870365708, 'eval_runtime': 195.5724, 'eval_samples_per_second': 0.256, 'eval_steps_per_second': 0.066, 'epoch': 0.23}\n",
      " 39%|█████████████                    | 13000/33000 [6:54:38<9:07:16,  1.64s/it]\n",
      "100%|███████████████████████████████████████████| 13/13 [03:09<00:00,  7.58s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-13000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-13000/special_tokens_map.json\n",
      "{'loss': 3.1549, 'grad_norm': 8.060042381286621, 'learning_rate': 3.015151515151515e-05, 'epoch': 0.23}\n",
      "{'loss': 3.2244, 'grad_norm': 10.519752502441406, 'learning_rate': 3e-05, 'epoch': 0.23}\n",
      "{'loss': 3.1285, 'grad_norm': 11.232885360717773, 'learning_rate': 2.9848484848484847e-05, 'epoch': 0.23}\n",
      "{'loss': 3.1404, 'grad_norm': 8.28058910369873, 'learning_rate': 2.96969696969697e-05, 'epoch': 0.23}\n",
      "{'loss': 3.1903, 'grad_norm': 7.1327338218688965, 'learning_rate': 2.954545454545455e-05, 'epoch': 0.24}\n",
      " 41%|█████████████▌                   | 13500/33000 [7:08:43<8:23:05,  1.55s/it]Saving model checkpoint to ./output/checkpoint-13500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-13500/special_tokens_map.json\n",
      "{'loss': 3.1639, 'grad_norm': 8.771100997924805, 'learning_rate': 2.9393939393939394e-05, 'epoch': 0.24}\n",
      "{'loss': 3.2249, 'grad_norm': 8.73171615600586, 'learning_rate': 2.9242424242424243e-05, 'epoch': 0.24}\n",
      "{'loss': 3.1294, 'grad_norm': 9.33590316772461, 'learning_rate': 2.909090909090909e-05, 'epoch': 0.24}\n",
      "{'loss': 3.1333, 'grad_norm': 8.28361988067627, 'learning_rate': 2.893939393939394e-05, 'epoch': 0.24}\n",
      "{'loss': 3.1964, 'grad_norm': 8.466567993164062, 'learning_rate': 2.878787878787879e-05, 'epoch': 0.24}\n",
      " 42%|██████████████                   | 14000/33000 [7:22:52<8:54:47,  1.69s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:07<00:39,  3.61s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:13<00:45,  4.53s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:21<00:52,  5.83s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:28<00:51,  6.49s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [00:36<00:48,  6.97s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [00:49<00:52,  8.75s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [01:45<01:58, 23.61s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [01:52<01:14, 18.58s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [02:02<00:47, 15.93s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [02:11<00:27, 13.92s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [02:17<00:11, 11.43s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.680795999999994, 'eval_rouge-2': 7.425842, 'eval_rouge-l': 24.969825999999998, 'eval_bleu-4': 0.03637226076792221, 'eval_runtime': 152.2498, 'eval_samples_per_second': 0.328, 'eval_steps_per_second': 0.085, 'epoch': 0.24}\n",
      " 42%|██████████████                   | 14000/33000 [7:25:24<8:54:47,  1.69s/it]\n",
      "100%|███████████████████████████████████████████| 13/13 [02:25<00:00, 10.17s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-14000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-14000/special_tokens_map.json\n",
      "{'loss': 3.174, 'grad_norm': 7.39209508895874, 'learning_rate': 2.863636363636364e-05, 'epoch': 0.25}\n",
      "{'loss': 3.1772, 'grad_norm': 7.038426876068115, 'learning_rate': 2.8484848484848486e-05, 'epoch': 0.25}\n",
      "{'loss': 3.1899, 'grad_norm': 8.231878280639648, 'learning_rate': 2.8333333333333335e-05, 'epoch': 0.25}\n",
      "{'loss': 3.1778, 'grad_norm': 7.6996684074401855, 'learning_rate': 2.818181818181818e-05, 'epoch': 0.25}\n",
      "{'loss': 3.1006, 'grad_norm': 7.87459659576416, 'learning_rate': 2.803030303030303e-05, 'epoch': 0.25}\n",
      " 44%|██████████████▌                  | 14500/33000 [7:39:32<8:10:13,  1.59s/it]Saving model checkpoint to ./output/checkpoint-14500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-14500/special_tokens_map.json\n",
      "{'loss': 3.182, 'grad_norm': 8.983089447021484, 'learning_rate': 2.7878787878787883e-05, 'epoch': 0.25}\n",
      "{'loss': 3.1811, 'grad_norm': 7.9663920402526855, 'learning_rate': 2.772727272727273e-05, 'epoch': 0.26}\n",
      "{'loss': 3.1127, 'grad_norm': 10.067524909973145, 'learning_rate': 2.7575757575757578e-05, 'epoch': 0.26}\n",
      "{'loss': 3.1811, 'grad_norm': 7.143675327301025, 'learning_rate': 2.7424242424242424e-05, 'epoch': 0.26}\n",
      "{'loss': 3.0633, 'grad_norm': 8.191901206970215, 'learning_rate': 2.7272727272727273e-05, 'epoch': 0.26}\n",
      " 45%|███████████████                  | 15000/33000 [7:53:44<7:41:52,  1.54s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:05<00:32,  2.91s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:13<00:48,  4.88s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:20<00:50,  5.58s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:28<00:51,  6.47s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [00:36<00:48,  6.92s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [00:46<00:48,  8.06s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [00:55<00:42,  8.41s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [01:03<00:32,  8.19s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [01:10<00:23,  7.86s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [01:19<00:16,  8.28s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [01:26<00:07,  7.72s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.196392, 'eval_rouge-2': 7.525124, 'eval_rouge-l': 26.334448, 'eval_bleu-4': 0.03922332879032424, 'eval_runtime': 99.4338, 'eval_samples_per_second': 0.503, 'eval_steps_per_second': 0.131, 'epoch': 0.26}\n",
      " 45%|███████████████                  | 15000/33000 [7:55:24<7:41:52,  1.54s/it]\n",
      "100%|███████████████████████████████████████████| 13/13 [01:32<00:00,  7.31s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-15000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-15000/special_tokens_map.json\n",
      "{'loss': 3.2025, 'grad_norm': 8.535135269165039, 'learning_rate': 2.7121212121212126e-05, 'epoch': 0.26}\n",
      "{'loss': 3.2092, 'grad_norm': 7.936419486999512, 'learning_rate': 2.696969696969697e-05, 'epoch': 0.27}\n",
      "{'loss': 3.1386, 'grad_norm': 9.264472007751465, 'learning_rate': 2.681818181818182e-05, 'epoch': 0.27}\n",
      "{'loss': 3.1605, 'grad_norm': 8.917896270751953, 'learning_rate': 2.6666666666666667e-05, 'epoch': 0.27}\n",
      "{'loss': 3.1963, 'grad_norm': 7.214958190917969, 'learning_rate': 2.6515151515151516e-05, 'epoch': 0.27}\n",
      " 47%|███████████████▌                 | 15500/33000 [8:09:36<9:51:41,  2.03s/it]Saving model checkpoint to ./output/checkpoint-15500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-15500/special_tokens_map.json\n",
      "{'loss': 3.1253, 'grad_norm': 8.029348373413086, 'learning_rate': 2.636363636363636e-05, 'epoch': 0.27}\n",
      "{'loss': 3.2293, 'grad_norm': 8.008797645568848, 'learning_rate': 2.6212121212121214e-05, 'epoch': 0.27}\n",
      "{'loss': 3.0949, 'grad_norm': 10.158119201660156, 'learning_rate': 2.6060606060606063e-05, 'epoch': 0.28}\n",
      "{'loss': 3.1726, 'grad_norm': 8.797774314880371, 'learning_rate': 2.590909090909091e-05, 'epoch': 0.28}\n",
      "{'loss': 3.1392, 'grad_norm': 9.528680801391602, 'learning_rate': 2.575757575757576e-05, 'epoch': 0.28}\n",
      " 48%|████████████████                 | 16000/33000 [8:23:38<7:07:06,  1.51s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:07<00:40,  3.71s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:13<00:48,  4.80s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [01:09<03:34, 23.84s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [02:04<04:38, 34.78s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [03:00<04:52, 41.73s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [03:11<03:10, 31.80s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [03:24<02:08, 25.78s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [03:34<01:23, 20.97s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [04:29<01:34, 31.42s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [04:37<00:48, 24.34s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [04:44<00:19, 19.03s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.164334, 'eval_rouge-2': 7.7857080000000005, 'eval_rouge-l': 24.617532, 'eval_bleu-4': 0.038591899658173896, 'eval_runtime': 347.1937, 'eval_samples_per_second': 0.144, 'eval_steps_per_second': 0.037, 'epoch': 0.28}\n",
      " 48%|████████████████                 | 16000/33000 [8:29:25<7:07:06,  1.51s/it]\n",
      "100%|███████████████████████████████████████████| 13/13 [04:51<00:00, 15.35s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-16000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-16000/special_tokens_map.json\n",
      "{'loss': 3.0912, 'grad_norm': 9.570995330810547, 'learning_rate': 2.5606060606060604e-05, 'epoch': 0.28}\n",
      "{'loss': 3.1537, 'grad_norm': 7.921268463134766, 'learning_rate': 2.5454545454545454e-05, 'epoch': 0.28}\n",
      "{'loss': 3.1798, 'grad_norm': 7.733911037445068, 'learning_rate': 2.5303030303030306e-05, 'epoch': 0.28}\n",
      "{'loss': 3.1649, 'grad_norm': 8.441293716430664, 'learning_rate': 2.5151515151515155e-05, 'epoch': 0.29}\n",
      "{'loss': 3.1591, 'grad_norm': 7.986569404602051, 'learning_rate': 2.5e-05, 'epoch': 0.29}\n",
      " 50%|████████████████▌                | 16500/33000 [8:43:32<8:39:38,  1.89s/it]Saving model checkpoint to ./output/checkpoint-16500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-16500/special_tokens_map.json\n",
      "{'loss': 3.155, 'grad_norm': 7.829233646392822, 'learning_rate': 2.4848484848484847e-05, 'epoch': 0.29}\n",
      "{'loss': 3.1552, 'grad_norm': 8.347114562988281, 'learning_rate': 2.46969696969697e-05, 'epoch': 0.29}\n",
      "{'loss': 3.1115, 'grad_norm': 8.513214111328125, 'learning_rate': 2.4545454545454545e-05, 'epoch': 0.29}\n",
      "{'loss': 3.1449, 'grad_norm': 7.2162394523620605, 'learning_rate': 2.4393939393939395e-05, 'epoch': 0.29}\n",
      "{'loss': 3.1228, 'grad_norm': 7.962463855743408, 'learning_rate': 2.4242424242424244e-05, 'epoch': 0.3}\n",
      " 52%|█████████████████                | 17000/33000 [8:57:40<6:46:33,  1.52s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:07<00:39,  3.56s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:13<00:45,  4.59s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:22<00:57,  6.44s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:29<00:53,  6.67s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [00:37<00:47,  6.86s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [00:47<00:48,  8.15s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [00:57<00:43,  8.63s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [01:06<00:34,  8.72s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [01:13<00:24,  8.11s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [01:20<00:15,  7.92s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [01:28<00:07,  7.83s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.45908, 'eval_rouge-2': 6.853524, 'eval_rouge-l': 25.867682, 'eval_bleu-4': 0.03376812949293659, 'eval_runtime': 100.1797, 'eval_samples_per_second': 0.499, 'eval_steps_per_second': 0.13, 'epoch': 0.3}\n",
      " 52%|█████████████████                | 17000/33000 [8:59:21<6:46:33,  1.52s/it]\n",
      "100%|███████████████████████████████████████████| 13/13 [01:34<00:00,  7.24s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-17000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-17000/special_tokens_map.json\n",
      "{'loss': 3.131, 'grad_norm': 7.50844669342041, 'learning_rate': 2.4090909090909093e-05, 'epoch': 0.3}\n",
      "{'loss': 3.1388, 'grad_norm': 8.541091918945312, 'learning_rate': 2.393939393939394e-05, 'epoch': 0.3}\n",
      "{'loss': 3.1919, 'grad_norm': 8.66935920715332, 'learning_rate': 2.3787878787878788e-05, 'epoch': 0.3}\n",
      "{'loss': 3.1579, 'grad_norm': 9.505596160888672, 'learning_rate': 2.3636363636363637e-05, 'epoch': 0.3}\n",
      "{'loss': 3.16, 'grad_norm': 8.9199800491333, 'learning_rate': 2.3484848484848487e-05, 'epoch': 0.31}\n",
      " 53%|█████████████████▌               | 17500/33000 [9:13:20<7:27:02,  1.73s/it]Saving model checkpoint to ./output/checkpoint-17500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-17500/special_tokens_map.json\n",
      "{'loss': 3.1239, 'grad_norm': 8.456695556640625, 'learning_rate': 2.3333333333333336e-05, 'epoch': 0.31}\n",
      "{'loss': 3.1326, 'grad_norm': 8.527462005615234, 'learning_rate': 2.318181818181818e-05, 'epoch': 0.31}\n",
      "{'loss': 3.17, 'grad_norm': 8.476994514465332, 'learning_rate': 2.3030303030303034e-05, 'epoch': 0.31}\n",
      "{'loss': 3.1821, 'grad_norm': 9.308837890625, 'learning_rate': 2.287878787878788e-05, 'epoch': 0.31}\n",
      "{'loss': 3.1495, 'grad_norm': 8.534646987915039, 'learning_rate': 2.272727272727273e-05, 'epoch': 0.31}\n",
      " 55%|██████████████████               | 18000/33000 [9:27:37<6:54:23,  1.66s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:08<00:48,  4.42s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:15<00:53,  5.38s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:25<01:02,  6.93s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:32<00:56,  7.03s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [00:41<00:53,  7.64s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [00:50<00:49,  8.31s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [01:00<00:43,  8.74s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [01:08<00:34,  8.55s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [01:16<00:25,  8.42s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [02:12<00:45, 22.69s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [02:18<00:17, 17.88s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.44770200000001, 'eval_rouge-2': 7.573807999999999, 'eval_rouge-l': 25.150641999999998, 'eval_bleu-4': 0.035892230375212, 'eval_runtime': 156.5022, 'eval_samples_per_second': 0.319, 'eval_steps_per_second': 0.083, 'epoch': 0.31}\n",
      " 55%|██████████████████               | 18000/33000 [9:30:13<6:54:23,  1.66s/it]\n",
      "100%|███████████████████████████████████████████| 13/13 [02:26<00:00, 14.54s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-18000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-18000/special_tokens_map.json\n",
      "{'loss': 3.1029, 'grad_norm': 8.747611045837402, 'learning_rate': 2.257575757575758e-05, 'epoch': 0.32}\n",
      "{'loss': 3.1621, 'grad_norm': 8.917329788208008, 'learning_rate': 2.2424242424242424e-05, 'epoch': 0.32}\n",
      "{'loss': 3.11, 'grad_norm': 8.88348388671875, 'learning_rate': 2.2272727272727274e-05, 'epoch': 0.32}\n",
      "{'loss': 3.1009, 'grad_norm': 8.806231498718262, 'learning_rate': 2.2121212121212123e-05, 'epoch': 0.32}\n",
      "{'loss': 3.1371, 'grad_norm': 8.601896286010742, 'learning_rate': 2.1969696969696972e-05, 'epoch': 0.32}\n",
      " 56%|██████████████████▌              | 18500/33000 [9:44:20<6:40:50,  1.66s/it]Saving model checkpoint to ./output/checkpoint-18500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-18500/special_tokens_map.json\n",
      "{'loss': 3.1436, 'grad_norm': 7.710687160491943, 'learning_rate': 2.1818181818181818e-05, 'epoch': 0.32}\n",
      "{'loss': 3.1288, 'grad_norm': 8.35131549835205, 'learning_rate': 2.1666666666666667e-05, 'epoch': 0.33}\n",
      "{'loss': 3.1571, 'grad_norm': 9.09300708770752, 'learning_rate': 2.1515151515151516e-05, 'epoch': 0.33}\n",
      "{'loss': 3.0544, 'grad_norm': 7.72016716003418, 'learning_rate': 2.1363636363636362e-05, 'epoch': 0.33}\n",
      "{'loss': 3.0908, 'grad_norm': 8.853437423706055, 'learning_rate': 2.1212121212121215e-05, 'epoch': 0.33}\n",
      " 58%|███████████████████              | 19000/33000 [9:58:25<6:19:02,  1.62s/it]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:07<00:42,  3.86s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:14<00:52,  5.27s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:22<00:54,  6.06s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:30<00:53,  6.74s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [00:39<00:51,  7.38s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [00:48<00:49,  8.17s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [01:00<00:46,  9.29s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [01:08<00:35,  8.89s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [01:16<00:25,  8.66s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [01:25<00:17,  8.64s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [02:20<00:22, 22.73s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.43211, 'eval_rouge-2': 8.294382, 'eval_rouge-l': 25.797852000000002, 'eval_bleu-4': 0.04309885569996474, 'eval_runtime': 153.5759, 'eval_samples_per_second': 0.326, 'eval_steps_per_second': 0.085, 'epoch': 0.33}\n",
      " 58%|██████████████████▍             | 19000/33000 [10:00:58<6:19:02,  1.62s/it]\n",
      "100%|███████████████████████████████████████████| 13/13 [02:26<00:00, 17.69s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-19000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-19000/special_tokens_map.json\n",
      "{'loss': 3.1365, 'grad_norm': 8.447800636291504, 'learning_rate': 2.106060606060606e-05, 'epoch': 0.33}\n",
      "{'loss': 3.1641, 'grad_norm': 7.829867839813232, 'learning_rate': 2.090909090909091e-05, 'epoch': 0.34}\n",
      "{'loss': 3.1541, 'grad_norm': 8.84961223602295, 'learning_rate': 2.075757575757576e-05, 'epoch': 0.34}\n",
      "{'loss': 3.1524, 'grad_norm': 8.657782554626465, 'learning_rate': 2.0606060606060608e-05, 'epoch': 0.34}\n",
      "{'loss': 3.2252, 'grad_norm': 10.181780815124512, 'learning_rate': 2.0454545454545457e-05, 'epoch': 0.34}\n",
      " 59%|██████████████████▉             | 19500/33000 [10:15:04<5:42:39,  1.52s/it]Saving model checkpoint to ./output/checkpoint-19500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./output/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-19500/special_tokens_map.json\n",
      " 59%|██████████████████▉             | 19562/33000 [10:16:51<6:33:54,  1.76s/it]"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python finetune_hf.py  data/AdvertiseGen_fix/  THUDM/chatglm3-6b  configs/lora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9418f6c5c264601",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 3. 使用微调的数据集进行推理\n",
    "在完成微调任务之后，我们可以查看到 `output` 文件夹下多了很多个`checkpoint-*`的文件夹，这些文件夹代表了训练的轮数。\n",
    "我们选择最后一轮的微调权重，并使用inference进行导入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f22b735175e1c0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:03:19.390123Z",
     "start_time": "2024-01-18T07:03:19.246666Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-1000   checkpoint-14500  checkpoint-19000  checkpoint-5500\n",
      "checkpoint-10000  checkpoint-1500   checkpoint-19500  checkpoint-6000\n",
      "checkpoint-10500  checkpoint-15000  checkpoint-2000   checkpoint-6500\n",
      "checkpoint-11000  checkpoint-15500  checkpoint-2500   checkpoint-7000\n",
      "checkpoint-11500  checkpoint-16000  checkpoint-3000   checkpoint-7500\n",
      "checkpoint-12000  checkpoint-16500  checkpoint-3500   checkpoint-8000\n",
      "checkpoint-12500  checkpoint-17000  checkpoint-4000   checkpoint-8500\n",
      "checkpoint-13000  checkpoint-17500  checkpoint-4500   checkpoint-9000\n",
      "checkpoint-13500  checkpoint-18000  checkpoint-500    checkpoint-9500\n",
      "checkpoint-14000  checkpoint-18500  checkpoint-5000\n"
     ]
    }
   ],
   "source": [
    "!ls output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26bfcebb-1612-44cf-8d1a-b9fedd1041f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/ChatGLM3/finetune_demo\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5060015c24e97ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:08:13.616364Z",
     "start_time": "2024-01-18T07:07:07.346906Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 /root/miniconda3/envs/chatglm3/bin/python inference_hf.py /root/ChatGLM3/finetune_demo/output/checkpoint-19000 --prompt \"类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd83087f096094",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 4. 总结\n",
    "到此位置，我们就完成了使用单张 GPU Lora 来微调 ChatGLM3-6B 模型，使其能生产出更好的广告。\n",
    "在本章节中，你将会学会：\n",
    "+ 如何使用模型进行 Lora 微调\n",
    "+ 微调数据集的准备和对齐\n",
    "+ 使用微调的模型进行推理"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
